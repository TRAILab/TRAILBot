import torch
from .model_loader import FCN8s, PSPNet
from PIL import Image as ImagePIL
from torchvision import transforms
import cv2

import rclpy
from rclpy.node import Node
from geometry_msgs.msg import PoseStamped
from sensor_msgs.msg import Image, PointCloud2
import sensor_msgs_py.point_cloud2 as pc2
import numpy as np
import math
from cv_bridge import CvBridge

import message_filters

'''
    The transformation matrix as well as the coordinate conversion and depth estimation functions are copied from human_detection_node
'''
camera_transformation_k = np.array([
    [628.5359544,0,676.9575694],
    [0,627.7249542,532.7206716],
    [0,0,1]])

rotation_matrix = np.array([
    [-0.007495781893,-0.0006277316155,0.9999717092],
    [-0.9999516401,-0.006361853422,-0.007499625104],
    [0.006366381192,-0.9999795662,-0.0005800141927]])

rotation_matrix = rotation_matrix.T

translation_vector = np.array([-0.06024059837, -0.08180891509, -0.3117851288])
image_width=1280
image_height=1024

def convert_to_lidar_frame(uv_coordinate):
    """
    convert 2d camera coordinate + depth into 3d lidar frame
    """
    point_cloud = np.empty( (3,) , dtype=float)
    point_cloud[2] = uv_coordinate[2]
    point_cloud[1] = ( image_height - uv_coordinate[1] )*point_cloud[2]
    point_cloud[0] = uv_coordinate[0]*point_cloud[2]

    inverse_camera_transformation_k = np.linalg.inv(camera_transformation_k)
    inverse_rotation_matrix = np.linalg.inv(rotation_matrix)
    point_cloud = inverse_camera_transformation_k @ point_cloud
    point_cloud = inverse_rotation_matrix @ (point_cloud-translation_vector) 
    return point_cloud

def convert_to_camera_frame(point_cloud):
    """
    convert 3d lidar data into 2d coordinate of the camera frame + depth
    """
    length = point_cloud.shape[0]
    translation = np.tile(translation_vector, (length, 1)).T
    
    point_cloud = point_cloud.T
    point_cloud = rotation_matrix@point_cloud + translation
    point_cloud = camera_transformation_k @ point_cloud

    uv_coordinate = np.empty_like(point_cloud)

    """
    uv = [x/z, y/z, z], and y is opposite so the minus imageheight
    """
    uv_coordinate[0] = point_cloud[0] / point_cloud[2]
    uv_coordinate[1] = image_height - point_cloud[1] / point_cloud[2]
    uv_coordinate[2] = point_cloud[2]

    uv_depth = uv_coordinate[2, :]
    filtered_uv_coordinate = uv_coordinate[:, uv_depth >= 0]
    return filtered_uv_coordinate

def estimate_depth(x, y, np_2d_array):
    """
    estimate the depth by finding points closest to x,y from thhe 2d array
    """
    # Calculate the distance between each point and the target coordinates (x, y)
    distances_sq = (np_2d_array[0,:] - x) ** 2 + (np_2d_array[1,:] - y) ** 2

    # Find the indices of the k nearest points
    k = 5     # Number of nearest neighbors we want
    closest_indices = np.argpartition(distances_sq, k)[:k]
    pixel_distance_threshold = 2000

    valid_indices = [idx for idx in closest_indices if distances_sq[idx]<=pixel_distance_threshold]
    if len(valid_indices) == 0:
        # lidar points disappears usually around 0.4m
        distance_where_lidar_stops_working = -1
        return distance_where_lidar_stops_working

    filtered_indices = np.array(valid_indices)
    # Get the depth value of the closest point
    closest_depths = np_2d_array[2,filtered_indices]

    return np.mean(closest_depths)

def load_model(device):
    # model = FCN8s(nclass=6, backbone='vgg16', pretrained_base=True, pretrained=True)
    model = PSPNet(nclass=2, backbone='resnet50', pretrained_base=True)
    model_location = 'psp2/psp_resnet50_pascal_voc_best_model.pth'
    model.load_state_dict(torch.load(f'src/trail_detection_node/trail_detection_node/model/{model_location}',map_location=torch.device('cuda:0')))
    model = model.to(device)
    model.eval()
    print('Finished loading model!')

    return model

def find_route(model, device, cv_image):
    PIL_image = ImagePIL.fromarray(cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB))
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    ])
    image = transform(PIL_image).unsqueeze(0).to(device)
    with torch.no_grad():
        output = model(image)
    # pred is prediction generated by the model, route is the variable for the center line
    pred = torch.argmax(output[0], 1).squeeze(0).cpu().data.numpy()
    pred[pred == 0] = 255 # only see the trail
    pred[pred == 1] = 0
    pred = np.array(pred, dtype=np.uint8)
    route = np.zeros_like(pred)
    # calculate the center line by taking the average
    row_num = 0
    for row in pred:
        white_pixels = list(np.nonzero(row)[0])
        if white_pixels:
            average = (white_pixels[0] + white_pixels[-1]) / 2
            route[row_num][round(average)] = 255
        row_num = row_num + 1
    return route

'''
    The traildetector node has two subscriptions(lidar and camera) and one publisher(trail position). After it receives msgs from both lidar and camera,
    it detects the trail in the image and sends the corresponding lidar position as the trail location.
    The msgs are synchornized before processing, using buffer and sync function.
    To find the path, the node will process the image, find a line to follow (by taking the average of left and right of the path), estimate the lidar points depth, 
    and choose to go to the closest point. 
    V1_traildetection assumes that the path is pointing frontward and has only one path in front.
'''
class trailDetector(Node):
    def __init__(self, model, device):
        super().__init__('trail_detector')
        # define trail publisher
        self.trail_publisher = self.create_publisher(
            PoseStamped,
            'trail_location',
            10)

        # create subscribers
        self.image_sub = message_filters.Subscriber(self, Image, 'camera')
        self.lidar_sub = message_filters.Subscriber(self, PointCloud2, 'velodyne_points')

        self.bridge = CvBridge()

        # load model and device
        self.model = model
        self.device = device

        # create callback
        queue_size = 30
        ts = message_filters.ApproximateTimeSynchronizer([self.image_sub, self.lidar_sub], queue_size, 0.5)
        ts.registerCallback(self.trail_callback)  
    
    def trail_callback(self, camera_msg, lidar_msg):
        print("Message received!")
        # process lidar msg
        point_gen = pc2.read_points(
            lidar_msg, field_names=(
                "x", "y", "z"), skip_nans=True)
        points = [[x, y, z] for x, y, z in point_gen]
        points = np.array(points)
        points2d = convert_to_camera_frame(points)

        # process camera msg
        cv_image = self.bridge.imgmsg_to_cv2(camera_msg, desired_encoding='passthrough')
        route = find_route(self.model, self.device, cv_image)
        route_indices = list(zip(*np.nonzero(route)))

        if not route_indices:
            print("No centerline found!")
            return

        #filter points that have no lidar points near it
        filtered_route_indices = []
        for index in route_indices:
            point = []
            u = index[1]
            v = image_height - index[0]
            point.append(u)
            point.append(v)
            point.append(estimate_depth(u, v, points2d))
            if point[2] == -1:
                continue
            else:
                filtered_route_indices.append(point)

        if not filtered_route_indices:
            print("No usable centerline found!")
            return

        # find the corresponding lidar points using the center line pixels
        filtered_3dPoints = []
        for index in filtered_route_indices:
            point = []
            point.append(index[0])
            point.append(index[1])
            point.append(index[2])
            point_3d = convert_to_lidar_frame(point)
            filtered_3dPoints.append(point_3d)

        filtered_3dPoints = np.array(filtered_3dPoints)
        # find the nearest 3d point and set that as goal
        distances_sq = filtered_3dPoints[:,0]**2 + filtered_3dPoints[:,1]**2 + filtered_3dPoints[:,2]**2
        smallest_index = np.argmin(distances_sq)

        # publsih message
        trail_location_msg = PoseStamped()
        trail_location_msg.header.stamp = lidar_msg.header.stamp
        trail_location_msg.header.frame_id = "velodyne"
        # position
        trail_location_msg.pose.position.x = filtered_3dPoints[smallest_index][0]  
        trail_location_msg.pose.position.y = filtered_3dPoints[smallest_index][1]
        trail_location_msg.pose.position.z = filtered_3dPoints[smallest_index][2]
        # orientation
        yaw = math.atan2(filtered_3dPoints[smallest_index][1], filtered_3dPoints[smallest_index][0])
        trail_location_msg.pose.orientation.x = 0.0  
        trail_location_msg.pose.orientation.y = 0.0 
        trail_location_msg.pose.orientation.z = math.sin(yaw/2)
        trail_location_msg.pose.orientation.w = math.cos(yaw / 2)
        self.trail_publisher.publish(trail_location_msg)
        
        # logging
        self.get_logger().info("location published!")
        self.get_logger().info(f"Point: {filtered_3dPoints[smallest_index][0]}, {filtered_3dPoints[smallest_index][1]}, {filtered_3dPoints[smallest_index][2]}")
        



def main(args=None):
    print(torch.cuda.is_available())
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = load_model(device)
    
    rclpy.init(args=args)
    trailDetectorNode = trailDetector(model, device)
    rclpy.spin(trailDetectorNode)

    trailDetectorNode.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()